Closing the Teacher-Forcing vs. Autoregressive Gap in Soft-Prompt Optimization

Background (Exposure Bias): The core issue is exposure bias – during training the model is teacher-forced on correct tokens, but at inference it conditions on its own (possibly flawed) outputs ￼. This train–test discrepancy leads to degenerate generations (loops, scrambled tokens) even when the prompt achieves low cross-entropy loss. To reliably reconstruct the target sequence, we need to modify the objective, training procedure, or decoding strategy to bridge this gap.

1. Sequence-Level Objectives (RL Fine-Tuning or Risk Minimization)

Instead of optimizing only token-level cross-entropy, use sequence-level training objectives that directly reward accurate full-sequence generation. For example:
	•	Reinforcement Learning (Policy Gradient): Treat the prompt as a policy and define a reward for exact reconstruction (e.g. +1 if the entire generated sequence matches the target, or a graded reward for partial matches). Then fine-tune the prompt with REINFORCE or PPO to maximize expected reward. This approach optimizes the model on the sequence level rather than stepwise, addressing exposure bias ￼. In practice, one can pre-train with cross-entropy and then alternate with RL fine-tuning (as in Ranzato et al. 2015’s MIXER algorithm ￼), gradually shifting from teacher-forced loss to sequence-level reward. By optimizing the actual generation outcome, RL can significantly reduce accumulated errors ￼.
	•	Minimum Risk / Sequence Likelihood Training: Instead of teacher-forced likelihood, directly maximize $P(\text{target sequence} \mid \text{prompt})$. This can be done by sampling or beam-searching complete outputs from the current prompt, and then adjusting the prompt embeddings to increase the log-probability of the correct sequence relative to incorrect ones. In effect, this is similar to structured prediction or minimum Bayes risk training: the prompt is updated to make the target sequence the unique high-probability outcome. In practice, one might sample a batch of sequences from the model given the prompt, compute a loss that penalizes any deviation from the target, and update the prompt accordingly (this is a form of sequence-level contrastive learning).
	•	Exact-Match Reward Shaping: Because exact match is a very sparse reward for longer sequences, it can help to shape the reward with intermediate objectives. For instance, give partial credit for generating correct prefixes or key tokens in the right order. This can smooth the optimization and guide the prompt towards the full correct sequence.

Why this helps: Sequence-level objectives close the train-test gap by training the prompt under the same conditions as inference (model feeding its own outputs). This directly addresses the problem that cross-entropy (teacher forcing) does not punish mistakes made during free generation. By optimizing the actual generation behavior, the prompt learns to avoid trajectories that diverge from the target sequence.

2. Scheduled Sampling (Mixing Teacher Forcing with Model Outputs)

Scheduled sampling is a curriculum strategy to gradually expose the model (and prompt) to its own predictions during training ￼ ￼. Instead of always feeding ground-truth tokens to the frozen LLM when computing loss, at some steps replace the ground-truth input with the model’s predicted token. Early in training you mostly use real tokens; as training progresses, you increasingly use the model’s own outputs as inputs ￼. This forces the prompt to learn to recover from slight off-target inputs and prevents the cascade of errors at inference. Key points:
	•	Use a schedule (e.g. linear or sigmoid decay) to control the probability of using a model-generated token at each time step ￼. Initially, the prompt learns under near teacher-forcing; later, it must perform under its own outputs, simulating inference conditions.
	•	Ensure that by the end of training, the model is often seeing its own one-step-off outputs and learning to correct course, thereby aligning training and inference dynamics. Recent research has refined this with confidence-based or dynamic schedules (using model confidence to decide when to switch to its output) ￼, but the basic idea remains to blend ground-truth and predicted tokens during prompt optimization ￼.
	•	Optionally, introduce prompt augmentation and EOS signals during scheduled sampling (the “prompt protection and EOS-prediction mechanisms” in some approaches ￼). For example, ensure an <EOS> token is part of the target sequence and include it in training; this helps the model learn when to terminate generation, avoiding endless loops past the target end.

Why this helps: Scheduled sampling directly mitigates exposure bias by making the training scenario more like inference ￼. The prompt will learn not just to predict the next token given perfect context, but also to stay on track when prior tokens might be slightly off. In other words, it teaches the prompt+model to handle its own mistakes, greatly reducing repetitive loops or token drift in practice.

3. Adversarial Hidden-State Alignment (Professor Forcing)

Another approach is inspired by Professor Forcing (Lamb et al., 2016), which uses adversarial training to align model behavior in training vs. inference modes. While the LLM is frozen, we can still apply the concept to prompt tuning:
	•	Hidden-State Distribution Matching: Run the model in two modes: (a) teacher-forced on the target sequence, and (b) autoregressive (free-running) using the prompt to generate a sequence (initially this free-run may diverge from target). Collect the hidden states (or logits) from both runs. Then train an auxiliary discriminator to tell apart teacher-forced vs. free-run hidden-state sequences ￼ ￼. Concurrently, update the prompt embeddings to fool the discriminator, i.e. make the model’s hidden state dynamics under free-run mode as close as possible to those under teacher-forcing ￼ ￼.
	•	Adversarial Loss: The prompt thus has a dual objective: (1) minimize the teacher-forced cross-entropy on the target (as before), and (2) minimize an adversarial loss so that the discriminator cannot distinguish whether the model is following the ground truth or its own outputs ￼. In effect, the prompt learns to keep the model in the trajectory of the target sequence even when generating freely. This reduces the chance of the model veering off into an abnormal state that produces gibberish or repeats.
	•	Practical implementation: Since the model is frozen, you are only tuning the prompt and possibly the discriminator. You might generate a free-run output with the current prompt (using greedy or sampling) up to the target length; if it’s not exact, the discriminator and prompt still get training signal. Over iterations, the prompt should shape the model’s free-run behavior closer to the teacher-forced behavior (which is the ideal target sequence). Essentially, this aligns the model’s internal state trajectory in inference mode with the trajectory it follows when perfectly guided ￼, closing the gap from the inside out.

Why this helps: By aligning hidden states, you ensure the model+prompt behaves coherently in free generation, not just in a teacher-forced setting. Professor Forcing was shown to reduce compounding errors and improve sequence coherence ￼. For prompt tuning, this means the optimized prompt will not only make the target likely under teacher forcing, but also constrain the autoregressive rollout to stay on the taught path.

4. Penalizing Degenerate Autoregressive Outputs (Unlikelihood Training)

To specifically tackle the observed failure modes (loops, token scrambling), you can incorporate a penalty term for undesired generations in the training objective:
	•	Unlikelihood Loss: Unlikelihood training (Li et al., 2020) adds a loss that explicitly punishes the model for assigning high probability to undesirable tokens or sequences ￼. For example, if the model (with the current prompt) tends to output “London” repeatedly, or produces a scrambled sequence, identify those tokens or n-grams that should not occur, and penalize their likelihood. Concretely, after each training iteration (or within each batch), you could run the prompt autoregressively to see common errors (e.g. it outputs token X instead of the correct token at position t). Then add an auxiliary loss $L_{ul}$ that lowers the probability of token X in that context. This drives the model to prefer the correct token and avoid the mistake. Over time, this eliminates attractions to repetitive or wrong outputs ￼.
	•	Repetition Penalty and Unique Token Constraints: A specific case of unlikelihood training is handling repetition loops. If the target is “London is the capital of England” but the model falls into “London London London…”, you can during training detect when a token is repeated beyond the target’s needs and apply a loss that penalizes the prompt/model for making $\Pr(\text{“London”}|,\text{“London … London”})$ high. This teaches the prompt to avoid triggering the model’s inherent tendency to repeat common words. Welleck et al. (2019) showed that the standard likelihood objective tends to over-produce frequent tokens and repeats, and that adding an unlikelihood objective fixes this, yielding “less repetitive, less dull text” while still allowing fluent generation ￼. In our case, that means the prompt would learn to strongly avoid the repetitive-loop attractor states.
	•	Negative Sampling of Incorrect Sequences: You can extend this idea by generating a few off-target sequences from the prompt and treating them as “negative samples.” The training can then maximize the probability of the target sequence while minimizing the probability of these common incorrect sequences (a sort of contrastive objective). This helps create a margin between the target and any competing high-probability outputs. For instance, if “ATSC” target yields a mistaken “SCSCATATSCATAT” output, include that output as a negative example so the prompt learns to drastically lower its probability. This pushes the model’s distribution such that only the exact target remains high-probability.

Why this helps: Standard cross-entropy only tells the model “make target likely,” but doesn’t explicitly say “don’t repeat a token” or “don’t generate in the wrong order.” Unlikelihood training and negative sampling provide that signal, actively suppressing the model’s probability for sequences that cause loops and scrambles ￼. This refines the prompt so that greedy decoding is funneled into the correct sequence and avoids known failure modes. Notably, unlikelihood-based tuning has been shown to yield better results with standard greedy/beam search decoding by fixing the model’s token-level probabilities ￼ – exactly what we need for reliable reconstruction.

5. Improved Inference Strategies (Decoding Adjustments)

Finally, modifications at inference time can complement the above training changes:
	•	Beam Search Over Greedy: Using a beam search decoder instead of purely greedy can often recover the correct sequence if the prompt has made it one of the high-probability outcomes rather than absolutely the highest at every step. Beam search explores multiple candidate continuations; if the greedy route falls into a repetitive loop with slightly higher token-level scores, the correct sequence might still be second-best locally but highest in overall sequence probability, and a beam search can find and output that sequence. In practice, increasing the beam width and using the model’s log-likelihood should allow the target sequence to win out, assuming the prompt training made the target nearly optimal. This doesn’t fix the training mismatch, but is a pragmatic way to avoid greedy getting stuck in a locally optimal loop.
	•	n-gram Repetition Penalties: During decoding, apply a penalty to probabilities of tokens that would create an n-gram repeat beyond a threshold. For example, a trigram blocking heuristic ensures the model never produces any 3-word sequence twice. If the model starts “London London…”, the decoder with trigram blocking will see that “London London” was already generated and reduce the probability of adding “London” again, prompting it to choose a different token. This can break infinite loops. Similarly, a softer repetition penalty (multiplying logits of already-used tokens by a factor < 1) will make repeats less likely. These techniques have been found effective in preventing degeneration in generation and can be applied at inference to enforce the structural constraint that the output shouldn’t keep looping.
	•	Constrained Decoding: If you know the target sequence or some structural aspects of it (e.g. a set of keywords or an expected format), you can guide decoding with constraints. For example, use a lexically constrained decoding algorithm that only allows generating words from the target sequence and in the correct order (this is a heavy constraint essentially forcing the output). A lighter approach is to bias the decoding towards the target n-grams: for instance, if the target is short and known, one could inject a bias at each step to prefer the next target token. However, these hard constraints might violate the spirit of using the prompt alone. Still, they highlight that the decoding process itself can be tuned to prefer the known target (since we do know exactly what we want to generate). In a research context, one might simulate this by checking at each step: if the model is about to output a wrong token, you could force the correct token – but this essentially falls back to teacher forcing at inference, so it’s mainly useful as a diagnostic or last-resort method.

Why this helps: While the ultimate goal is to have the prompt itself drive correct output, inference-time measures like beam search and repetition penalties are straightforward ways to avoid known pitfalls without retraining. Beam search ensures we consider the global sequence probability (mitigating myopic greedy choices), and repetition or constraint techniques ensure we don’t allow the model’s inherent flaws (like looping on high-frequency tokens) to manifest. These strategies have been shown to improve generation quality even in vanilla models ￼ and would likely boost reliability in our prompt-tuned scenario as well. The key is that any remaining teacher-forcing gap issues can be partially counteracted by a smarter decoder so that exact reconstruction is achieved at inference.

⸻

In summary, to reliably get exact target sequences from a frozen 7B LLM via learned prompts, we must make the training conditions mirror the generation conditions. This can be done by mixing model-predicted tokens into training (scheduled sampling) ￼, directly optimizing for sequence-level correctness (e.g. with RL to handle the non-differentiable exact-match objective) ￼, and using objectives that penalize divergence or degenerate loops (adversarial alignment and unlikelihood losses) ￼ ￼. In practice, a combination of these approaches often works best: e.g. begin with teacher-forced prompt tuning, then apply scheduled sampling, and finally fine-tune with a small RL or unlikelihood phase to iron out autoregressive errors. By also employing robust decoding (beam search with repetition penalties), we ensure that the soft prompt+LLM system consistently produces the desired sequence with greedy or beam decoding at test time. Each of these modifications attacks the teacher-forcing vs. free-running gap from a different angle, and together they can substantially close that gap, yielding reliable exact reconstructions of target texts.

Sources:
	•	Exposure bias from teacher-forced training vs. autoregressive inference ￼
	•	Scheduled sampling gradually transitions to model’s own outputs to reduce this gap ￼ ￼
	•	Hybrid training mixing teacher forcing and free-running improves robustness ￼
	•	RL fine-tuning directly optimizes sequence-level metrics, addressing accumulated error ￼
	•	Ranzato et al. (2015) – sequence-level training via mixed cross-entropy and REINFORCE ￼
	•	Professor Forcing aligns hidden state dynamics in training vs. generation (adversarially) ￼
	•	Unlikelihood training penalizes model for high-probability repeats/incorrect outputs, reducing loops ￼
	•	Likelihood objectives can over-weight frequent or repeat tokens; unlikelihood fixes this, yielding better generation with standard decoding ￼